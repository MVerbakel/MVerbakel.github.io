---
layout: post
title: "Metrics"
mathjax: true
---

Based on the book [Trustworthy Online Controlled Experiments: a practical guide to A/B testing](https://www.amazon.com/Trustworthy-Online-Controlled-Experiments-Practical/dp/1108724264) by Kohavi, Tang, and Xu.



- **Goals** (north star): Based on a description of what the company ultimately cares about (why do you exist?), these are metrics that should capture progress towards success. They might be hard to move, requiring many initiatives, or a longer time period to observe change. It also might not be possible to perfectly capture the goal with metrics, so these might be proxies, and require iteration over time. However, these should be simple (easily understood and accepted) and stable (not updated every feature launch).
- **Drivers** (sign-post/surrogate /predictive metrics): Metrics that are thought to be drivers of success (goals), indicating you're moving in the right direction. For example, to increase revenue, we need to acquire customers, engage, and retain them. These should be aligned with goals, actionable and relevant, sensitive, and resistant to gaming (incentivise the right behaviours). Inspiration can be taken from the user funnel, or frameworks such as Google's [HEART framework](https://research.google/pubs/pub36299/): Happiness (attitudinal: satisfaction, willingness to recommend, perceived ease of use); Engagement (behavioral proxies like frequency, intensity, or depth of interaction over some time period, e.g. photos uploaded per user per day); Adoption (e.g. accounts created within 7 days); Retention (e.g. percentage of 7-day active users in a given week who are still 7-day active users 3 months later); and, Task Success (efficiency or time to complete a task, effectiveness or percent of tasks completed, and error rate). Another framework is AARRR!: Acquisition, Activation, Retention, Referral, and Revenue.
- **Guardrails** (protection metrics): Either protect the business or protect the trustworthiness of the experiment. For example, in a password management company security might be the goal, but ease of use and accessibility might be guardrails. Other common guardrails are performance metrics (e.g. load time, errors, crashes), costs like customer service tickets, and metrics that are not expected to change (e.g. pageviews per user). These could also be the driver or goal metrics of other teams (e.g. a team working on something like relevance might want to keep an eye on some form of revenue per user metric, ensuring it's sensitive enough). For a more holistic example, checkout how Airbnb has approached guardrails [here](https://medium.com/airbnb-engineering/designing-experimentation-guardrails-ed6a976ec669). Though [apparently](https://newsletter.bringthedonuts.com/p/building-products-at-airbnb) it's still a heavily debated topic internally. 
- **Diagnosis or debug metrics**: When the above metrics show a problem, these provide more granularity for drilling down. E.g. for click-through-rate, you might have the CTR of certain areas of the page, and for revenue you might have a binary indicator (1 if made any purchase), and the average revenue only for those where there is a purchase (i.e. the components of average revenue per visitor).  

Other taxonomies are briefly discussed: 1) assets (e.g. total Facebook accounts or connections) vs engagement (e.g. sessions or pageviews) metrics; and, 2) business (e.g. revenue per user, or daily active users, tracking business health) vs operational metrics (e.g. queries per second, tracking operational concerns).

#### Tips when developing business metrics:

- Less scalable methods like user research can be used to generate ideas, which can then be validated with scalable analyses to get a precise definition. For example, we may see bounce rate (spending a very short time on the page) is correlated with dissatisfaction, then do an analysis of the distribution to find an exact cutoff that seems reasonable (e.g. 20 seconds).
- Consider measures of quality to help interpret driver metrics: For example, if a user clicks but then clicks back shortly after, this might be considered a bad click; a good profile on LinkedIn might be one with 'sufficient' information filled in. 
-  If a model is used, it needs to be interpretable and regularly validated. For example, a popular metric is Lifetime Value (LTV), based on the predicted future revenue for a user. For experiments, Netflix uses bucketed watch hours as it's easily understood and indicative of long-term retention.
- Sometimes it's easier to measure what you don't want. For example, a short visit after clicking a search result is usually more correlated with an unhappy user, whereas a long visit could mean they're happy (found what they want) or are unhappy and having a tough time finding what they want.
- Remember metrics are proxies, and the drivers and goals should be hard to game. For example, CTR might be used to measure engagement, but driving CTR alone could just increase clickbait. Similarly, short term revenue can be increased by increasing prices or ads, but these likely negatively harm customer retention. Additional metrics are needed to cover the edge cases, e.g. using human evaluation to measure relevance. Further, constrained metrics are less gameable e.g. ad revenue constrained by space on the page.
- Avoid vanity metrics and focus on user value and actions instead. For example, counting banner ads is not that useful (users can ignore), whereas counting 'likes' captures a user action (and is a fundamental part of the experience). 
- Be careful not to over pivot towards what is measurable, and in particular what is measurable in the short term. Other important factors like how customers feel about you product are very important, but difficult to measure. Taking into account multiple sources of information when making decisions can help mitigate this.