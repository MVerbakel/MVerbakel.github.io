---
layout: post
title: "Variance reduction in experiments"
mathjax: true
---

Expand on the methods and how CUPED works etc.



Ways to reduce variance:

- **Use a binary metric**: If still aligned well with the hypothesis, we could use a binary metric instead of a high variance continuous one. E.g. the number of searches has a higher variance than the number of searchers (1 if any search made, else 0). Similarly, Netflix uses a binary indicator of whether the user streamed more than X hours in a time period instead of measuring average streaming hours.
- **Cap/winsorize/transform**: If your metric is prone to outliers (unrelated to treatment), winsorizing (capping) values at some percentile will reduce variance (e.g. find the 1st and 99th percentile for the combined data, then cap values above/below those in both samples). 
- **Use stratification**: Divide the population into strata (e.g. based on platform, browser, day of week, country), then sample within each stratum separately, combining the results for the overall estimate. When sampling, we expect some random differences between the groups, so this is a way to reduce the differences (reducing variance). This is usually done at the time of sampling, but can also be done retrospectively during the analysis.
- **Use covariates**: Let's say you're at Amazon and you want to increase supply. You might run an experiment where you incentivise shops to add new products, and use the number of products per shop as a success metric. Before entering the experiment, some shops have only a few products, while others have thousands. Controlling for their pre-treatment number of products likely reduces the variance, increasing power and our ability to detect smaller effects. We do this by fitting a regression, controlling for data known pre-experiment. The variance from pre-experiment factors is unrelated to the treatment, so we can safely remove it without introducing bias. The Microsoft experiment team developed the method CUPED (Controlled-experiment using Pre-Experiment Data) for exactly this ([paper](https://www.exp-platform.com/Documents/2013-02-CUPED-ImprovingSensitivityOfControlledExperiments.pdf), and an [article](https://booking.ai/how-booking-com-increases-the-power-of-online-experiments-with-cuped-995d186fff1d#:~:text=CUPED%20is%20a%20technique%20developed,power%20to%20detect%20small%20effects.) on how Booking.com uses this method. Ideally, the covariate is the same as the experiment metric but measured on the pre-experiment period (as the variance reduction is based on the correlation between the covariate and the metric).
- **Use a more granular unit of observation**: If possible, using a more granular ID will increase the sample size substantially (e.g. instead of user, you could use page views or searches). However, this is often difficult to do without any interaction or leakage (e.g. changing a page could have flow-on effects for the user's experience on other pages).
- **Paired experiment**: If it's possible to test both variant and control for the same user, a paired design will remove the between-user variability. For example, we can test two ranked lists at the same time with a design called interleaving (see this Netflix [post](https://netflixtechblog.com/interleaving-in-online-experiments-at-netflix-a04ee392ec55) for examples).
- **Pooled control group**: If you have multiple experiments running at the same time, each with their own control group, you might be able to combine them to increase the sample size and power.