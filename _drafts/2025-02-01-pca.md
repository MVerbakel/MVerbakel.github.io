---
layout: post
title: "Principal Components Analysis (PCA)"
mathjax: true
---

Create a smaller set of features (dimensionality reduction), visualise how clusters relate (using the first 2/3 components as the axis), 


Uses: dimensionality reduction (create features)


Eigenvectors are often referred to as loadings as they give the amount of each feature which is included in each component. Multiplying the original data (nxp) by the eigenvectors gives the final components (nxk).

The process is: 1) standardise each feature by subtracting the mean and dividing by the standard deviation (so features on a larger scale don't dominate the variance); 2) compute the covariance matrix; 3) compute the eigenvectors (direction of variance) and eigenvalues (amount of variance) of the covariance matrix; 4) sort the eigenvectors by their corresponding eigenvalues in descending order. The eigenvectors with the highest eigenvalues are the principal components that capture the most variance in the data.


In PCA, the loadings represent the contributions (or weights) of the original variables (features) to each of the principal components. These loadings tell you how each original feature contributes to the principal components.


Maybe add Partial least squares in this post as a comparison