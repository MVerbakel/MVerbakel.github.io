---
layout: post
title: "What is a p value?"
mathjax: true
---

Statistical tests use a p-value to communicate the significance of the result. It might seem simple at first, but the definition tends to confuse people: the p-value is the probability of obtaining a result at least as extreme as the result observed, when the null hypothesis is true (i.e. when there is no effect, or no difference). In other words, if there was really no effect, how surprising is the result? If the p-value is very low, it's very uncommon to see a result like this when the null is true, so we take it as evidence towards the null being false. However, we haven't proven anything, but rather use our level of 'surprise' to guide our decision. 

You might be wondering how small is small enough? We have to pre-define a threshold at which point we will reject the null. The smaller the threshold, the lower the false positive rate, but the larger the sample needed to detect an effect when it exists. A common choice is 0.05, which means we reject the null when the p-value is less than 0.05, accepting a false positive rate of 5% (i.e. 1/20 times we reject the null when it's actually true).

This is an essential part of hypothesis testing, so it's worth breaking that down a bit and covering some of the common misunderstandings:

### Remember it's conditioned on the null hypothesis being true

$$ p value = P(\text{observed difference} \:|\: H_\text{o} true)$$

Let's say we are comparing means. If we were to repeat our experiment many times (n times), with no effect between the groups (i.e. like an AA experiment, where both groups get the same control treatment), we will get n results for the difference between the means. If we plot those results, we can see the range of results we might get when the null is true (called the sampling distribution). Note, with large samples, the sampling distribution will converge to a normal distribution, so long as the variance is finite (according to CLT). So we generally standardise the results and compare to the standard normal curve instead of actually doing the repeated sampling (or t-distribution for t-test). 

You might be familiar with the 68-95-99.7 rule, which applied here tells us 68% of the results with be within ~1 standard deviation of the mean (0, or no difference), 95% within ~2 standard deviation, and 99.7% within ~3. By setting alpha to 0.05, we want 95% of the data to be less than our absolute threshold, so we reject the null when the result is greater than or equal to +/- 1.96 standard deviations from the mean (alpha/2=0.25 in each tail).  

![Sampling distributions](/assets/standard_normal_distribution.png)

Now we just need to locate where our result falls in this distribution. The p-value will simply be the area under the curve in the tail, when we cut at our observed value. From the plot, if the cut is more extreme or equal to the significance threshold, we will reject the null. Otherwise, we fail to reject the null (it could be true or not). 

![Sampling distributions](/assets/sampling_distribution_tails.png)

### Understand that it's not the probability of the null being true

Some get confused and think it's the probability that the null hypothesis is true. This is incorrect, as we stated above, it's conditioned on the null being true. 

To calculate the probability the null hypothesis is true, given some observed difference, we also need the prior probability the null hypothesis is true:

$$ P( H_\text{o}\:\text{true}  \:|\: \text{observed difference}) = \frac{P(\text{observed difference} \:|\: H_\text{o}\:\text{true}) * P(H_\text{o}\:\text{true})}{P(\text{observed difference})} $$

$$ = \frac{P(H_\text{o} true)}{P(\text{observed difference})} * pvalue $$

It may help to think about the extreme case where we only run AA experiments. In this case, the null hypothesis is true 100% of the time. Inversely, if all of our experiments are effective, then the null is true 0% of time. However, in both cases, we can still run a hypothesis test and assess the evidence based on the probability of the result, assuming that the null hypothesis is true. 

Other variations of this include:
- The p-value is the probability the result is due to chance. This is missing the important caveat: assuming the null is true. It is the probability that by chance we would see a result at least as extreme as this, when the null is true.
- The inverse of the p value (i.e. 1-p) is the probability the alternate is true. As we're conditioning on the null being true, we are only looking at possible outcomes when the null is true. So the inverse would be the probability of a less extreme result (the rest of the distribution), again assuming the null is true. We never prove the alternate is true, but rather gather evidence against of whether it's likely that the null is true (if it's unlikely, we reject the null and accept the alternate).

### The p-value and confidence interval always agree

If the p-value is significant, the confidence interval for the difference won't overlap with the null (i.e. 0 if the null is no difference), and vice versa. Assuming they're generated by the same hypothesis test, and significance level. The reason is the significance level determines the distance from the estimated effect to the upper and lower limit of the confidence interval, as well as the distance from the null to the critical regions. 

Just remember, we're looking at the confidence interval of the difference, not of each sample. If we compare the confidence interval around the estimate for each sample, it can still be significant even if they overlap.

### Final thoughts

- One reason for the misunderstanding is that the p-value doesn't deliver what most people really want to know: the probability that the hypothesis is correct. An alternative might be using a bayesian framework, but this is far less common. 
- Does it really matter? There can be a big difference between the probability that the null hypothesis is true and the p-value, so it does make a difference in the interpretation.
- A good way to explain it to a non-technical audience is focusing the concept of 'surprise'. If we start off thinking the null hypothesis is true, how surprised are we by the results? If we're surprised, it might be enough to convince us to take some decision. 
- However, we haven't proven anything, and there is always some risk of a false positive. So depending on our risk tolerance, we should adjust our bar for 'surprise' accordingly, and ensure we follow a scientific method. After all, with a 5% false positive rate we can eventually get a significant result by just running many tests (i.e. p-hacking). 
- If we see multiple sources of evidence (like repeated experiments) that all show a surprising result, we can therefore be more confident in our decision. An analogy for this is, if you see one bad review about hygiene for a restaurant you like, it may not be enough to change your opinion. However, if you see multiple credible reviews of bad hygiene, at some point there will be enough evidence for you to change your opinion and decide not to return.
- In addition to statistical significance, we also need to consider the practical significance of the result. When are working with large samples, even very small differences can be statistically significant. So it's important to consider the p-value in combination with other information. For example, if a change results in a 0.01% change, the expected return may not be enough to cover the cost or effort involved with changing. For a customer facing metric, let's say average weight of a cereal box, a 0.01 gram difference is probably not enough for a customer to notice or care about even if it is significant statistically.

References: [Wikipedia](https://en.wikipedia.org/wiki/P-value)