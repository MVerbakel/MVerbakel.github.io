---
layout: post
title: "What is a p value?"
mathjax: true
---

The p-value might seem simple at first, but the definition tends to confuse people. Formally, the p-value is the probability of obtaining a result at least as extreme as the result observed, when the null hypothesis is true (i.e. when there is no effect, or no difference). Less formally, if we start off thinking the null is true (no effect), how surprised are we by the result? If we're sufficiently surprised, we reject the null and go with the alternative hypothesis. 

In practice, imagine we ran many A/A experiments, where both groups receive no treatment (i.e. we know the null is true, there is no effect). This tells us the range of possible values to expect under the null. Comparing an observed treatment effect to that distribution, we can calculate how often we observe an effect at least as extreme as that (i.e. the p-value). If the p-value is very low, it's very uncommon to see a result like this when the null is true, so we take it as evidence towards the null being false. 

**Keep in mind we haven't proven anything, but rather use our level of 'surprise' to guide our decision.** There is always some risk of a false positive (incorrectly rejecting the null). Therefore, depending on our risk tolerance, we should adjust our bar for 'surprise' accordingly and ensure we follow a scientific method.

## A few points to remember:

### 1. Remember it's conditioned on the null hypothesis being true

$$ p value = P(\text{observed difference} \:|\: H_\text{o} true)$$

Let's say we are comparing means. We can estimate the distribution under the null with a normal or t distribution (based on CLT the distribution of the mean difference will converge to normal with large samples so long as the variance is finite). You might be familiar with the 68-95-99.7 rule, which applied here tells us 68% of the results with be within ~1 standard deviation of the mean (0, or no difference), 95% within ~2 standard deviation, and 99.7% within ~3. By setting alpha to 0.05, we want 95% of the data to be less than our absolute threshold, so we reject the null when the result is greater than or equal to +/- 1.96 standard deviations from the mean (alpha/2=0.25 or 2.5% in each tail).  

![Sampling distributions](/assets/standard_normal_distribution.png)

Now we just need to locate where our result falls in this distribution. The p-value will simply be the area under the curve in the tail, when we cut at our observed value. From the plot, if the cut is more extreme or equal to the significance threshold, we will reject the null. Otherwise, we fail to reject the null (it could be true or not). 

![Sampling distributions](/assets/sampling_distribution_tails.png)

### 2. The p-value is not the probability of the null being true

Some get confused and think the p-value is the probability that the null hypothesis is true. However, as we noted above it's conditioned on the null being true, so this is incorrect. It may help to think about the extreme case where we only run A/A experiments. In this case, the null hypothesis is true 100% of the time. Inversely, if all of our experiments are effective, then the null is true 0% of time. However, in both cases, we can still run a hypothesis test and assess the evidence based on the probability of the result, assuming that the null hypothesis is true. 

To calculate the probability the null hypothesis is true, given some observed difference, we would also need the prior probability the null hypothesis is true:

$$ P( H_\text{o}\:\text{true}  \:|\: \text{observed difference}) = \frac{P(\text{observed difference} \:|\: H_\text{o}\:\text{true}) * P(H_\text{o}\:\text{true})}{P(\text{observed difference})} $$

$$ = \frac{P(H_\text{o} true)}{P(\text{observed difference})} * pvalue $$

### 3. The inverse of the p-value (1-p) is not the probability the alternate is true

As we're conditioning on the null being true, we are only looking at possible outcomes when the null is true. So the inverse would be the probability of a less extreme result (the rest of the distribution), again assuming the null is true. We never prove the alternate is true with a hypothesis test, but rather gather evidence against of whether it's likely that the null is true. If it's unlikely, we reject the null and accept the alternate.

### 4. The results depend on the significance level chosen

You might be wondering how small is small enough for us to conclude the null is 'unlikely' enough to accept the alternate? The answer is not so satisfying, we have to pre-define a threshold at which point we will reject the null. The smaller the threshold, the lower the false positive rate, but the larger the sample needed to detect an effect when it exists. There will always be a trade-off. A common choice is 0.05, which means we reject the null when the p-value is less than 0.05, accepting a false positive rate of 5% (i.e. 1/20 times we reject the null when it's actually true).

### 5. The p-value and confidence interval always agree

If the p-value is significant, the confidence interval for the difference won't overlap with the null (i.e. 0 if the null is no difference), and vice versa. Assuming they're generated by the same hypothesis test, and significance level. The reason is the significance level determines the distance from the estimated effect to the upper and lower limit of the confidence interval, as well as the distance from the null to the critical regions. 

Just remember, we're looking at the confidence interval of the difference, not of each sample. If we compare the confidence interval around the estimate for each sample, it can still be significant even if they overlap.

## Final thoughts

- One reason for the misunderstanding is that the p-value doesn't deliver what most people really want to know: the probability the hypothesis is correct. For that we would need a bayesian approach.
- The p-value can easily be abused so it gets a bad reputation. If we run many tests, we'll eventually get a false positive. 
- We can counter this by following the scientific method and considering the p-value in combination with other information (other sources of evidence, and the practical significance of the result).

References: [Wikipedia](https://en.wikipedia.org/wiki/P-value)